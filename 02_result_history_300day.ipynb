{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import and process Dow Jon Futures history \n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "pf = pd.read_json(\"historyIndex.json\")\n",
    "#add one more label column in the each row (Last day)\n",
    "futuresData = pd.DataFrame(columns=pf['dataset']['column_names'], data = pf['dataset']['data'])\n",
    "futuresData = futuresData.iloc[::-1]\n",
    "futuresData.index = range(len(futuresData))\n",
    "y = futuresData.loc[1:2630, ['Last']]\n",
    "y.index = range(len(y))\n",
    "y.columns = ['Next Day Last']\n",
    "futuresData = pd.merge(futuresData, y, left_index=True, right_index=True)\n",
    "futuresData.drop(['Change'], axis = 1, inplace = True, errors = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-12-26</td>\n",
       "      <td>0.1316</td>\n",
       "      <td>0.7986</td>\n",
       "      <td>0.0699</td>\n",
       "      <td>-0.22393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-12-27</td>\n",
       "      <td>0.0928</td>\n",
       "      <td>0.8173</td>\n",
       "      <td>0.0901</td>\n",
       "      <td>0.06701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-12-28</td>\n",
       "      <td>0.1055</td>\n",
       "      <td>0.8434</td>\n",
       "      <td>0.0509</td>\n",
       "      <td>-0.19519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-12-29</td>\n",
       "      <td>0.1509</td>\n",
       "      <td>0.8358</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>-0.23619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-12-30</td>\n",
       "      <td>0.1047</td>\n",
       "      <td>0.8389</td>\n",
       "      <td>0.0564</td>\n",
       "      <td>-0.13520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2011-12-31</td>\n",
       "      <td>0.1224</td>\n",
       "      <td>0.7984</td>\n",
       "      <td>0.0793</td>\n",
       "      <td>-0.05395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>0.1673</td>\n",
       "      <td>0.7635</td>\n",
       "      <td>0.0693</td>\n",
       "      <td>-0.22919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2012-01-02</td>\n",
       "      <td>0.1994</td>\n",
       "      <td>0.7566</td>\n",
       "      <td>0.0440</td>\n",
       "      <td>-0.41134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2012-01-03</td>\n",
       "      <td>0.1593</td>\n",
       "      <td>0.8201</td>\n",
       "      <td>0.0206</td>\n",
       "      <td>-0.33805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2012-01-04</td>\n",
       "      <td>0.1741</td>\n",
       "      <td>0.8038</td>\n",
       "      <td>0.0221</td>\n",
       "      <td>-0.24281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>0.2186</td>\n",
       "      <td>0.7276</td>\n",
       "      <td>0.0537</td>\n",
       "      <td>-0.38887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2012-01-06</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>0.8206</td>\n",
       "      <td>0.0450</td>\n",
       "      <td>-0.23281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2012-01-07</td>\n",
       "      <td>0.1825</td>\n",
       "      <td>0.7515</td>\n",
       "      <td>0.0661</td>\n",
       "      <td>-0.19485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2012-01-08</td>\n",
       "      <td>0.1756</td>\n",
       "      <td>0.7762</td>\n",
       "      <td>0.0480</td>\n",
       "      <td>-0.28807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2012-01-09</td>\n",
       "      <td>0.1853</td>\n",
       "      <td>0.7026</td>\n",
       "      <td>0.1121</td>\n",
       "      <td>-0.15282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2012-01-10</td>\n",
       "      <td>0.1867</td>\n",
       "      <td>0.7517</td>\n",
       "      <td>0.0616</td>\n",
       "      <td>-0.38912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2012-01-11</td>\n",
       "      <td>0.0434</td>\n",
       "      <td>0.8943</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.02702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2012-01-12</td>\n",
       "      <td>0.1328</td>\n",
       "      <td>0.8032</td>\n",
       "      <td>0.0640</td>\n",
       "      <td>-0.14211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2012-01-13</td>\n",
       "      <td>0.1007</td>\n",
       "      <td>0.8331</td>\n",
       "      <td>0.0663</td>\n",
       "      <td>-0.07778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2012-01-14</td>\n",
       "      <td>0.1138</td>\n",
       "      <td>0.7417</td>\n",
       "      <td>0.1445</td>\n",
       "      <td>-0.04549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2012-01-15</td>\n",
       "      <td>0.1651</td>\n",
       "      <td>0.7003</td>\n",
       "      <td>0.1348</td>\n",
       "      <td>-0.09491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2012-01-16</td>\n",
       "      <td>0.1525</td>\n",
       "      <td>0.7897</td>\n",
       "      <td>0.0579</td>\n",
       "      <td>-0.20628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2012-01-17</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.8187</td>\n",
       "      <td>0.0558</td>\n",
       "      <td>-0.12339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2012-01-18</td>\n",
       "      <td>0.1197</td>\n",
       "      <td>0.7934</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>-0.05640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2012-01-19</td>\n",
       "      <td>0.2223</td>\n",
       "      <td>0.7506</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>-0.28812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2012-01-20</td>\n",
       "      <td>0.1742</td>\n",
       "      <td>0.7352</td>\n",
       "      <td>0.0905</td>\n",
       "      <td>-0.19602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2012-01-21</td>\n",
       "      <td>0.1203</td>\n",
       "      <td>0.8382</td>\n",
       "      <td>0.0415</td>\n",
       "      <td>-0.32834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2012-01-22</td>\n",
       "      <td>0.1598</td>\n",
       "      <td>0.7723</td>\n",
       "      <td>0.0680</td>\n",
       "      <td>-0.12253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2012-01-23</td>\n",
       "      <td>0.1824</td>\n",
       "      <td>0.7650</td>\n",
       "      <td>0.0527</td>\n",
       "      <td>-0.24584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2012-01-24</td>\n",
       "      <td>0.1545</td>\n",
       "      <td>0.7729</td>\n",
       "      <td>0.0726</td>\n",
       "      <td>-0.28032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2271</th>\n",
       "      <td>2018-03-15</td>\n",
       "      <td>0.2735</td>\n",
       "      <td>0.6629</td>\n",
       "      <td>0.0638</td>\n",
       "      <td>-0.35954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2272</th>\n",
       "      <td>2018-03-16</td>\n",
       "      <td>0.0970</td>\n",
       "      <td>0.7901</td>\n",
       "      <td>0.1130</td>\n",
       "      <td>-0.03306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2273</th>\n",
       "      <td>2018-03-17</td>\n",
       "      <td>0.1571</td>\n",
       "      <td>0.7743</td>\n",
       "      <td>0.0687</td>\n",
       "      <td>-0.17443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2274</th>\n",
       "      <td>2018-03-18</td>\n",
       "      <td>0.1746</td>\n",
       "      <td>0.7621</td>\n",
       "      <td>0.0633</td>\n",
       "      <td>-0.27514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2275</th>\n",
       "      <td>2018-03-19</td>\n",
       "      <td>0.1282</td>\n",
       "      <td>0.7849</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>-0.06474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2276</th>\n",
       "      <td>2018-03-20</td>\n",
       "      <td>0.1158</td>\n",
       "      <td>0.8187</td>\n",
       "      <td>0.0655</td>\n",
       "      <td>-0.15053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2277</th>\n",
       "      <td>2018-03-21</td>\n",
       "      <td>0.1486</td>\n",
       "      <td>0.8354</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>-0.20886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2278</th>\n",
       "      <td>2018-03-22</td>\n",
       "      <td>0.3807</td>\n",
       "      <td>0.6193</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.58831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2279</th>\n",
       "      <td>2018-03-23</td>\n",
       "      <td>0.1328</td>\n",
       "      <td>0.8218</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>-0.17221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2280</th>\n",
       "      <td>2018-03-24</td>\n",
       "      <td>0.1237</td>\n",
       "      <td>0.7993</td>\n",
       "      <td>0.0770</td>\n",
       "      <td>-0.08462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2281</th>\n",
       "      <td>2018-03-25</td>\n",
       "      <td>0.2480</td>\n",
       "      <td>0.6953</td>\n",
       "      <td>0.0568</td>\n",
       "      <td>-0.31635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2282</th>\n",
       "      <td>2018-03-26</td>\n",
       "      <td>0.1917</td>\n",
       "      <td>0.7680</td>\n",
       "      <td>0.0402</td>\n",
       "      <td>-0.28184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2283</th>\n",
       "      <td>2018-03-27</td>\n",
       "      <td>0.1918</td>\n",
       "      <td>0.7977</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>-0.33007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2284</th>\n",
       "      <td>2018-03-28</td>\n",
       "      <td>0.2131</td>\n",
       "      <td>0.7016</td>\n",
       "      <td>0.0854</td>\n",
       "      <td>-0.27523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2285</th>\n",
       "      <td>2018-03-29</td>\n",
       "      <td>0.0668</td>\n",
       "      <td>0.8695</td>\n",
       "      <td>0.0637</td>\n",
       "      <td>0.04495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2286</th>\n",
       "      <td>2018-03-30</td>\n",
       "      <td>0.2003</td>\n",
       "      <td>0.7216</td>\n",
       "      <td>0.0780</td>\n",
       "      <td>-0.21031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2287</th>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>0.2263</td>\n",
       "      <td>0.6541</td>\n",
       "      <td>0.1196</td>\n",
       "      <td>-0.17401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2288</th>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>0.1877</td>\n",
       "      <td>0.7804</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>-0.35547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2289</th>\n",
       "      <td>2018-04-02</td>\n",
       "      <td>0.1202</td>\n",
       "      <td>0.8130</td>\n",
       "      <td>0.0669</td>\n",
       "      <td>-0.10667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290</th>\n",
       "      <td>2018-04-03</td>\n",
       "      <td>0.1457</td>\n",
       "      <td>0.7274</td>\n",
       "      <td>0.1269</td>\n",
       "      <td>-0.11664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2291</th>\n",
       "      <td>2018-04-04</td>\n",
       "      <td>0.0917</td>\n",
       "      <td>0.8444</td>\n",
       "      <td>0.0639</td>\n",
       "      <td>-0.06000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2292</th>\n",
       "      <td>2018-04-05</td>\n",
       "      <td>0.1458</td>\n",
       "      <td>0.7960</td>\n",
       "      <td>0.0581</td>\n",
       "      <td>-0.12365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2293</th>\n",
       "      <td>2018-04-06</td>\n",
       "      <td>0.1803</td>\n",
       "      <td>0.7864</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>-0.31217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2294</th>\n",
       "      <td>2018-04-07</td>\n",
       "      <td>0.1292</td>\n",
       "      <td>0.8269</td>\n",
       "      <td>0.0439</td>\n",
       "      <td>-0.21204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2295</th>\n",
       "      <td>2018-04-08</td>\n",
       "      <td>0.2209</td>\n",
       "      <td>0.7567</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>-0.36989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296</th>\n",
       "      <td>2018-04-09</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.7770</td>\n",
       "      <td>0.1132</td>\n",
       "      <td>-0.10704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2297</th>\n",
       "      <td>2018-04-10</td>\n",
       "      <td>0.2074</td>\n",
       "      <td>0.7501</td>\n",
       "      <td>0.0425</td>\n",
       "      <td>-0.24805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>2018-04-11</td>\n",
       "      <td>0.2152</td>\n",
       "      <td>0.7537</td>\n",
       "      <td>0.0311</td>\n",
       "      <td>-0.31850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2299</th>\n",
       "      <td>2018-04-12</td>\n",
       "      <td>0.2159</td>\n",
       "      <td>0.7408</td>\n",
       "      <td>0.0433</td>\n",
       "      <td>-0.29647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2300</th>\n",
       "      <td>2018-04-13</td>\n",
       "      <td>0.2336</td>\n",
       "      <td>0.7202</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>-0.36933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2301 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date     neg     neu     pos  compound\n",
       "0     2011-12-26  0.1316  0.7986  0.0699  -0.22393\n",
       "1     2011-12-27  0.0928  0.8173  0.0901   0.06701\n",
       "2     2011-12-28  0.1055  0.8434  0.0509  -0.19519\n",
       "3     2011-12-29  0.1509  0.8358  0.0132  -0.23619\n",
       "4     2011-12-30  0.1047  0.8389  0.0564  -0.13520\n",
       "5     2011-12-31  0.1224  0.7984  0.0793  -0.05395\n",
       "6     2012-01-01  0.1673  0.7635  0.0693  -0.22919\n",
       "7     2012-01-02  0.1994  0.7566  0.0440  -0.41134\n",
       "8     2012-01-03  0.1593  0.8201  0.0206  -0.33805\n",
       "9     2012-01-04  0.1741  0.8038  0.0221  -0.24281\n",
       "10    2012-01-05  0.2186  0.7276  0.0537  -0.38887\n",
       "11    2012-01-06  0.1343  0.8206  0.0450  -0.23281\n",
       "12    2012-01-07  0.1825  0.7515  0.0661  -0.19485\n",
       "13    2012-01-08  0.1756  0.7762  0.0480  -0.28807\n",
       "14    2012-01-09  0.1853  0.7026  0.1121  -0.15282\n",
       "15    2012-01-10  0.1867  0.7517  0.0616  -0.38912\n",
       "16    2012-01-11  0.0434  0.8943  0.0623   0.02702\n",
       "17    2012-01-12  0.1328  0.8032  0.0640  -0.14211\n",
       "18    2012-01-13  0.1007  0.8331  0.0663  -0.07778\n",
       "19    2012-01-14  0.1138  0.7417  0.1445  -0.04549\n",
       "20    2012-01-15  0.1651  0.7003  0.1348  -0.09491\n",
       "21    2012-01-16  0.1525  0.7897  0.0579  -0.20628\n",
       "22    2012-01-17  0.1257  0.8187  0.0558  -0.12339\n",
       "23    2012-01-18  0.1197  0.7934  0.0869  -0.05640\n",
       "24    2012-01-19  0.2223  0.7506  0.0271  -0.28812\n",
       "25    2012-01-20  0.1742  0.7352  0.0905  -0.19602\n",
       "26    2012-01-21  0.1203  0.8382  0.0415  -0.32834\n",
       "27    2012-01-22  0.1598  0.7723  0.0680  -0.12253\n",
       "28    2012-01-23  0.1824  0.7650  0.0527  -0.24584\n",
       "29    2012-01-24  0.1545  0.7729  0.0726  -0.28032\n",
       "...          ...     ...     ...     ...       ...\n",
       "2271  2018-03-15  0.2735  0.6629  0.0638  -0.35954\n",
       "2272  2018-03-16  0.0970  0.7901  0.1130  -0.03306\n",
       "2273  2018-03-17  0.1571  0.7743  0.0687  -0.17443\n",
       "2274  2018-03-18  0.1746  0.7621  0.0633  -0.27514\n",
       "2275  2018-03-19  0.1282  0.7849  0.0869  -0.06474\n",
       "2276  2018-03-20  0.1158  0.8187  0.0655  -0.15053\n",
       "2277  2018-03-21  0.1486  0.8354  0.0160  -0.20886\n",
       "2278  2018-03-22  0.3807  0.6193  0.0000  -0.58831\n",
       "2279  2018-03-23  0.1328  0.8218  0.0453  -0.17221\n",
       "2280  2018-03-24  0.1237  0.7993  0.0770  -0.08462\n",
       "2281  2018-03-25  0.2480  0.6953  0.0568  -0.31635\n",
       "2282  2018-03-26  0.1917  0.7680  0.0402  -0.28184\n",
       "2283  2018-03-27  0.1918  0.7977  0.0105  -0.33007\n",
       "2284  2018-03-28  0.2131  0.7016  0.0854  -0.27523\n",
       "2285  2018-03-29  0.0668  0.8695  0.0637   0.04495\n",
       "2286  2018-03-30  0.2003  0.7216  0.0780  -0.21031\n",
       "2287  2018-03-31  0.2263  0.6541  0.1196  -0.17401\n",
       "2288  2018-04-01  0.1877  0.7804  0.0318  -0.35547\n",
       "2289  2018-04-02  0.1202  0.8130  0.0669  -0.10667\n",
       "2290  2018-04-03  0.1457  0.7274  0.1269  -0.11664\n",
       "2291  2018-04-04  0.0917  0.8444  0.0639  -0.06000\n",
       "2292  2018-04-05  0.1458  0.7960  0.0581  -0.12365\n",
       "2293  2018-04-06  0.1803  0.7864  0.0333  -0.31217\n",
       "2294  2018-04-07  0.1292  0.8269  0.0439  -0.21204\n",
       "2295  2018-04-08  0.2209  0.7567  0.0222  -0.36989\n",
       "2296  2018-04-09  0.1098  0.7770  0.1132  -0.10704\n",
       "2297  2018-04-10  0.2074  0.7501  0.0425  -0.24805\n",
       "2298  2018-04-11  0.2152  0.7537  0.0311  -0.31850\n",
       "2299  2018-04-12  0.2159  0.7408  0.0433  -0.29647\n",
       "2300  2018-04-13  0.2336  0.7202  0.0462  -0.36933\n",
       "\n",
       "[2301 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import and process the sentiment of the reddit sumissions title\n",
    "sentimentData = pd.read_csv(\"mean_sentiment14_date.csv\")\n",
    "sentimentData.columns=['Date','neg','neu','pos','compound']\n",
    "sentimentData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014-01-24\n",
      "2016-11-04\n",
      "2018-01-17\n",
      "            Date     Open     High      Low    Settle    Volume  \\\n",
      "0     2012-11-13  12777.0  12855.0  12684.0  12717.00  138310.0   \n",
      "1     2012-11-14  12731.0  12803.0  12509.0  12544.00  167420.0   \n",
      "2     2012-11-15  12545.0  12581.0  12464.0  12522.00  152927.0   \n",
      "3     2012-11-16  12520.0  12573.0  12434.0  12570.00  167779.0   \n",
      "4     2012-11-19  12579.0  12764.0  12573.0  12738.00  126099.0   \n",
      "5     2012-11-20  12733.0  12777.0  12666.0  12759.00  122895.0   \n",
      "6     2012-11-21  12757.0  12813.0  12696.0  12800.00   82562.0   \n",
      "7     2012-11-23  12802.0  12982.0  12801.0  12961.00   61917.0   \n",
      "8     2012-11-26  12949.0  12951.0  12870.0  12936.00   90154.0   \n",
      "9     2012-11-27  12940.0  12971.0  12845.0  12862.00  125648.0   \n",
      "10    2012-11-28  12867.0  12967.0  12740.0  12957.00  171456.0   \n",
      "11    2012-11-29  12961.0  13049.0  12939.0  13021.00  136755.0   \n",
      "12    2012-11-30  12987.0  13056.0  12973.0  13008.00  112994.0   \n",
      "13    2012-12-03  13003.0  13080.0  12930.0  12950.00  121995.0   \n",
      "14    2012-12-04  12940.0  13008.0  12914.0  12933.00  116192.0   \n",
      "15    2012-12-05  12919.0  13076.0  12905.0  13024.00  173092.0   \n",
      "16    2012-12-06  13015.0  13073.0  12997.0  13063.00  106044.0   \n",
      "17    2012-12-07  13070.0  13154.0  13037.0  13143.00  141385.0   \n",
      "18    2012-12-10  13147.0  13194.0  13103.0  13187.00   89273.0   \n",
      "19    2012-12-11  13169.0  13304.0  13152.0  13278.00  115980.0   \n",
      "20    2012-12-12  13275.0  13326.0  13221.0  13230.00  141546.0   \n",
      "21    2012-12-13  13221.0  13269.0  13145.0  13157.00  136657.0   \n",
      "22    2012-12-14  13157.0  13216.0  13116.0  13150.00   50307.0   \n",
      "23    2012-12-17  13188.0  13255.0  13140.0  13245.00   39599.0   \n",
      "24    2012-12-18  13246.0  13362.0  13226.0  13329.00   33897.0   \n",
      "25    2012-12-19  13331.0  13374.0  13229.0  13268.00   34263.0   \n",
      "26    2012-12-20  13227.0  13324.0  13218.0  13322.00   15834.0   \n",
      "27    2012-12-21  13312.0  13313.0  13021.0  13249.52    2121.0   \n",
      "28    2012-12-24  13133.0  13136.0  13060.0  13065.00   22366.0   \n",
      "29    2012-12-26  13094.0  13111.0  13011.0  13048.00   37860.0   \n",
      "...          ...      ...      ...      ...       ...       ...   \n",
      "1324  2018-02-21  24951.0  25257.0  24724.0  24782.00  254310.0   \n",
      "1325  2018-02-22  24783.0  25137.0  24576.0  25013.00  276284.0   \n",
      "1326  2018-02-23  25019.0  25325.0  24996.0  25314.00  203948.0   \n",
      "1327  2018-02-26  25328.0  25774.0  25300.0  25758.00  189537.0   \n",
      "1328  2018-02-27  25758.0  25813.0  25388.0  25429.00  308510.0   \n",
      "1329  2018-02-28  25411.0  25577.0  25018.0  25038.00  270530.0   \n",
      "1330  2018-03-01  25090.0  25185.0  24431.0  24620.00  450881.0   \n",
      "1331  2018-03-02  24616.0  24670.0  24204.0  24535.00  397142.0   \n",
      "1332  2018-03-05  24483.0  24955.0  24311.0  24864.00  309660.0   \n",
      "1333  2018-03-06  24885.0  25047.0  24695.0  24852.00  262663.0   \n",
      "1334  2018-03-07  24659.0  24845.0  24410.0  24795.00  313894.0   \n",
      "1335  2018-03-08  24784.0  24956.0  24704.0  24892.00  228978.0   \n",
      "1336  2018-03-09  24879.0  25346.0  24841.0  25335.00  153047.0   \n",
      "1337  2018-03-12  25335.0  25510.0  25151.0  25208.00  116773.0   \n",
      "1338  2018-03-13  25196.0  25378.0  24945.0  25027.00  102167.0   \n",
      "1339  2018-03-14  24985.0  25153.0  24670.0  24770.00   83440.0   \n",
      "1340  2018-03-15  24784.0  25059.0  24675.0  24907.00   60747.0   \n",
      "1341  2018-03-16  24921.0  24948.0  24823.0  24879.17    2557.0   \n",
      "1342  2018-03-19  24976.0  24984.0  24461.0  24691.00  272836.0   \n",
      "1343  2018-03-20  24671.0  24813.0  24608.0  24765.00  190952.0   \n",
      "1344  2018-03-21  24769.0  24998.0  24660.0  24727.00  225022.0   \n",
      "1345  2018-03-22  24732.0  24803.0  23932.0  23963.00  408367.0   \n",
      "1346  2018-03-23  23999.0  24103.0  23496.0  23612.00  482624.0   \n",
      "1347  2018-03-26  23609.0  24223.0  23551.0  24191.00  321534.0   \n",
      "1348  2018-03-27  24201.0  24435.0  23691.0  23859.00  380005.0   \n",
      "1349  2018-03-28  23854.0  24075.0  23707.0  23860.00  490323.0   \n",
      "1350  2018-03-29  23889.0  24288.0  23797.0  24147.00  297290.0   \n",
      "1351  2018-04-02  24097.0  24147.0  23306.0  23552.00  355871.0   \n",
      "1352  2018-04-03  23609.0  24010.0  23545.0  23984.00  354073.0   \n",
      "1353  2018-04-04  23981.0  24282.0  23361.0  24265.00  381535.0   \n",
      "\n",
      "      Previous Day Open Interest  Next Day Last  \n",
      "0                       101101.0       12544.00  \n",
      "1                       101005.0       12522.00  \n",
      "2                       101319.0       12570.00  \n",
      "3                       100484.0       12738.00  \n",
      "4                        94345.0       12759.00  \n",
      "5                        95051.0       12800.00  \n",
      "6                        95921.0       12961.00  \n",
      "7                        96069.0       12936.00  \n",
      "8                        94842.0       12862.00  \n",
      "9                        95474.0       12957.00  \n",
      "10                       95845.0       13021.00  \n",
      "11                       92127.0       13008.00  \n",
      "12                       93114.0       12950.00  \n",
      "13                       89500.0       12933.00  \n",
      "14                       91217.0       13024.00  \n",
      "15                       93873.0       13063.00  \n",
      "16                       94626.0       13143.00  \n",
      "17                       91375.0       13187.00  \n",
      "18                       92332.0       13278.00  \n",
      "19                       91799.0       13230.00  \n",
      "20                       93532.0       13157.00  \n",
      "21                       92483.0       13150.00  \n",
      "22                       80256.0       13245.00  \n",
      "23                       68203.0       13329.00  \n",
      "24                       54628.0       13268.00  \n",
      "25                       48005.0       13322.00  \n",
      "26                       46878.0       13249.52  \n",
      "27                       47600.0       13065.00  \n",
      "28                       96228.0       13048.00  \n",
      "29                       96020.0       13003.00  \n",
      "...                          ...            ...  \n",
      "1324                    116491.0       25031.00  \n",
      "1325                    115895.0       25300.00  \n",
      "1326                    116277.0       25759.00  \n",
      "1327                    115278.0       25427.00  \n",
      "1328                    113457.0       25090.00  \n",
      "1329                    116354.0       24638.00  \n",
      "1330                    111656.0       24539.00  \n",
      "1331                    112246.0       24881.00  \n",
      "1332                    115615.0       24851.00  \n",
      "1333                    111013.0       24790.00  \n",
      "1334                    111841.0       24886.00  \n",
      "1335                    110747.0       25340.00  \n",
      "1336                    106230.0       25206.00  \n",
      "1337                     90071.0       25010.00  \n",
      "1338                     77318.0       24784.00  \n",
      "1339                     57743.0       24938.00  \n",
      "1340                     49985.0       24881.00  \n",
      "1341                     41425.0       24675.00  \n",
      "1342                    106972.0       24774.00  \n",
      "1343                    106977.0       24728.00  \n",
      "1344                    106080.0       24001.00  \n",
      "1345                    106660.0       23586.00  \n",
      "1346                    107565.0       24202.00  \n",
      "1347                    106316.0       23847.00  \n",
      "1348                    102806.0       23870.00  \n",
      "1349                    103075.0       24096.00  \n",
      "1350                    103579.0       23620.00  \n",
      "1351                     98611.0       23981.00  \n",
      "1352                    101366.0       24243.00  \n",
      "1353                     96215.0       24480.00  \n",
      "\n",
      "[1354 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "#merge imported data\n",
    "dataset = sentimentData.merge(futuresData, on='Date', how='left')\n",
    "dataset = dataset.dropna(how='any')  #TODO\n",
    "dataset.index = range(len(dataset))\n",
    "\n",
    "dataset = dataset.drop([\"Last\"],axis=1) #TODO\n",
    "dataset = dataset.drop([\"neg\"], axis=1)\n",
    "dataset = dataset.drop([\"neu\"], axis=1)\n",
    "dataset = dataset.drop([\"pos\"], axis=1)\n",
    "dataset = dataset.drop([\"compound\"], axis=1)\n",
    "\n",
    "print(dataset[\"Date\"][300])\n",
    "print(dataset[\"Date\"][1000])\n",
    "print(dataset[\"Date\"][1300])\n",
    "\n",
    "print (dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 12777.  12855.  12684. ... 138310. 101101.  12544.]\n",
      " [ 12731.  12803.  12509. ... 167420. 101005.  12522.]\n",
      " [ 12545.  12581.  12464. ... 152927. 101319.  12570.]\n",
      " ...\n",
      " [ 24097.  24147.  23306. ... 355871.  98611.  23981.]\n",
      " [ 23609.  24010.  23545. ... 354073. 101366.  24243.]\n",
      " [ 23981.  24282.  23361. ... 381535.  96215.  24480.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#define constant variable, hidden layer units and learn rate\n",
    "rnnUnit = 10      \n",
    "inputSize = 6      \n",
    "outputSize = 1\n",
    "learningRate = 0.0006    \n",
    "\n",
    "#training data\n",
    "data = dataset.iloc[:,1:8].values\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get training dataset\n",
    "def getTrainingData( batchSize, timeStep, trainBegin, trainEnd, data):\n",
    "    batch_index = []\n",
    "    data_train = data[trainBegin:trainEnd]\n",
    "    mean = np.mean(data_train, axis=0)\n",
    "    std = np.std(data_train, axis=0)\n",
    "    \n",
    "    # Stardardization\n",
    "    normalized_train_data = (data_train-mean)/std  \n",
    "    train_x,train_y = [],[]   \n",
    "    for i in range(len(normalized_train_data)-timeStep):\n",
    "       if i % batchSize == 0:\n",
    "           batch_index.append(i)\n",
    "       x=normalized_train_data[i:i+timeStep, :inputSize]\n",
    "       y=normalized_train_data[i:i+timeStep, inputSize, np.newaxis]\n",
    "       train_x.append(x.tolist())\n",
    "       train_y.append(y.tolist())\n",
    "    batch_index.append((len(normalized_train_data)-timeStep))\n",
    "    return batch_index, train_x, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get test dataset\n",
    "def getTestData( timeStep, testBegin, trainEnd, data):\n",
    "    data_test = data[testBegin:trainEnd]\n",
    "    mean = np.mean(data_test, axis=0)\n",
    "    std = np.std(data_test,axis=0)\n",
    "    \n",
    "    # Stardardization\n",
    "    normalized_test_data=(data_test-mean)/std  \n",
    "    size=(len(normalized_test_data)+timeStep-1)//timeStep  #有size个sample\n",
    "        \n",
    "    test_x,test_y=[],[]\n",
    "    for i in range(size-1): \n",
    "       x=normalized_test_data[i*timeStep:(i+1)*timeStep, :inputSize]\n",
    "       y=normalized_test_data[i*timeStep:(i+1)*timeStep, inputSize]\n",
    "       test_x.append(x.tolist())\n",
    "       test_y.extend(y)\n",
    "    \n",
    "        \n",
    "    test_x.append((normalized_test_data[(i+1)*timeStep:, :inputSize]).tolist())\n",
    "    test_y.extend((normalized_test_data[(i+1)*timeStep:, inputSize]).tolist())\n",
    "     \n",
    "    return mean, std, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights={\n",
    "        'in':tf.Variable(tf.random_normal([inputSize,rnnUnit])),\n",
    "        'out':tf.Variable(tf.random_normal([rnnUnit,1]))\n",
    "}\n",
    "biases={\n",
    "        'in':tf.Variable(tf.constant(0.1,shape=[rnnUnit,])),\n",
    "        'out':tf.Variable(tf.constant(0.1,shape=[1,]))\n",
    "}\n",
    "\n",
    "#LSTM cell\n",
    "def lstm(X):   \n",
    "    \n",
    "    #define neural networks parameters\n",
    "    #define input/ouput weight, bias\n",
    "   \n",
    "    batchSize = tf.shape(X)[0]\n",
    "    timeStep = tf.shape(X)[1]\n",
    "    w_in = weights['in']\n",
    "    b_in = biases['in']\n",
    "    input = tf.reshape(X,[-1, inputSize])  \n",
    "    input_rnn = tf.matmul(input,w_in)+b_in\n",
    "    input_rnn = tf.reshape(input_rnn,[-1, timeStep, rnnUnit])  \n",
    "    cell = tf.contrib.rnn.core_rnn_cell.BasicLSTMCell(rnnUnit, reuse = tf.get_variable_scope().reuse )\n",
    "    init_state = cell.zero_state(batchSize, dtype=tf.float32)\n",
    "    with tf.variable_scope(\"rnn\", reuse = None):\n",
    "        output_rnn, final_states=tf.nn.dynamic_rnn(cell, input_rnn, initial_state=init_state, dtype=tf.float32)\n",
    "    \n",
    "    output = tf.reshape(output_rnn,[-1,rnnUnit]) \n",
    "    w_out = weights['out']\n",
    "    b_out = biases['out']\n",
    "    pred = tf.matmul(output,w_out)+b_out\n",
    "    return pred, final_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train LSTM\n",
    "def trainLSTM( batchSize, timeStep, trainBegin, trainEnd, data):\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape=[None, timeStep, inputSize])\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, timeStep, outputSize])\n",
    "    batch_index, train_x, train_y = getTrainingData( batchSize, timeStep, trainBegin, trainEnd, data)\n",
    "    \n",
    "    with tf.variable_scope(\"lstm4\"):\n",
    "        pred, _ = lstm(X)\n",
    "        \n",
    "    loss = tf.reduce_mean(tf.square(tf.reshape(pred,[-1])-tf.reshape(Y, [-1])))\n",
    "    train_op = tf.train.AdamOptimizer(learningRate).minimize(loss)\n",
    "    saver = tf.train.Saver(tf.global_variables(), max_to_keep=15)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        #number of iteration\n",
    "        for i in range(1000):     \n",
    "            for step in range(len(batch_index)-1):\n",
    "                _, loss_ = sess.run([train_op, loss], feed_dict={ X: train_x[batch_index[step]:batch_index[step+1]], Y: train_y[batch_index[step]:batch_index[step+1]]})\n",
    "            print(\"Number of iterations:\", i,\" loss:\", loss_)\n",
    "        print(\"Model saves: \", saver.save(sess, './model4.ckpt'))\n",
    "        print(\"The training is finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Model prediction\n",
    "def prediction(timeStep, testBegin, trainEnd, data):\n",
    "    X = tf.placeholder(tf.float32, shape=[None, timeStep, inputSize])\n",
    "    mean, std, test_x, test_y = getTestData(timeStep, testBegin, trainEnd, data)\n",
    "    \n",
    "    #print(len(test_x))\n",
    "    #print(test_x)\n",
    "    #print(len(test_y))    \n",
    "    \n",
    "    with tf.variable_scope(\"lstm4\", reuse=True):\n",
    "        pred, _ = lstm(X)\n",
    "           \n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    with tf.Session() as sess: \n",
    "        #read parameters\n",
    "        module_file = tf.train.latest_checkpoint('./')\n",
    "        saver.restore(sess, module_file)\n",
    "        test_predict = []\n",
    "        \n",
    "        for step in range(len(test_x)-1):\n",
    "          prob = sess.run( pred, feed_dict={X:[test_x[step]]} )\n",
    "          #pprint([test_x[step]]) \n",
    "          predict=prob.reshape((-1))\n",
    "          #pprint(predict)\n",
    "          test_predict.extend(predict)\n",
    "        \n",
    "        test_y = np.array(test_y)*std[inputSize]+mean[inputSize]\n",
    "        test_predict = np.array(test_predict)*std[inputSize]+mean[inputSize]\n",
    "        \n",
    "        \n",
    "\n",
    "        PMSE = np.average(np.abs(test_predict[20:]-test_y[20:len(test_predict)])/test_y[20:len(test_predict)])  \n",
    "        print(\"PMSE:\", PMSE)\n",
    "             \n",
    "        RMSE = np.sqrt(((test_predict[20:]-test_y[20:len(test_predict)]) ** 2).mean())    \n",
    "        print(\"RMSE:\", RMSE)        \n",
    "          \n",
    "        plt.figure()\n",
    "        xAxis = range(20, len(test_predict));\n",
    "        plt.plot(xAxis, test_predict[20:], '-bo',)\n",
    "        plt.plot(xAxis, test_y[20:len(test_predict)], '-ro')      \n",
    "        plt.show()\n",
    "        \n",
    "    return test_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 0  loss: 23.33327\n",
      "Number of iterations: 1  loss: 20.344889\n",
      "Number of iterations: 2  loss: 17.618628\n",
      "Number of iterations: 3  loss: 15.034513\n",
      "Number of iterations: 4  loss: 12.466879\n",
      "Number of iterations: 5  loss: 9.852897\n",
      "Number of iterations: 6  loss: 7.3394055\n",
      "Number of iterations: 7  loss: 5.2209363\n",
      "Number of iterations: 8  loss: 3.6504831\n",
      "Number of iterations: 9  loss: 2.6745346\n",
      "Number of iterations: 10  loss: 2.1262665\n",
      "Number of iterations: 11  loss: 1.8050016\n",
      "Number of iterations: 12  loss: 1.6003041\n",
      "Number of iterations: 13  loss: 1.4604588\n",
      "Number of iterations: 14  loss: 1.3578933\n",
      "Number of iterations: 15  loss: 1.277041\n",
      "Number of iterations: 16  loss: 1.2089591\n",
      "Number of iterations: 17  loss: 1.1485416\n",
      "Number of iterations: 18  loss: 1.0929979\n",
      "Number of iterations: 19  loss: 1.0409696\n",
      "Number of iterations: 20  loss: 0.9919604\n",
      "Number of iterations: 21  loss: 0.9459263\n",
      "Number of iterations: 22  loss: 0.9029726\n",
      "Number of iterations: 23  loss: 0.8631557\n",
      "Number of iterations: 24  loss: 0.8263877\n",
      "Number of iterations: 25  loss: 0.79243714\n",
      "Number of iterations: 26  loss: 0.7609814\n",
      "Number of iterations: 27  loss: 0.73168033\n",
      "Number of iterations: 28  loss: 0.70422465\n",
      "Number of iterations: 29  loss: 0.67836165\n",
      "Number of iterations: 30  loss: 0.6538996\n",
      "Number of iterations: 31  loss: 0.63069516\n",
      "Number of iterations: 32  loss: 0.60863787\n",
      "Number of iterations: 33  loss: 0.587638\n",
      "Number of iterations: 34  loss: 0.56761783\n",
      "Number of iterations: 35  loss: 0.54850644\n",
      "Number of iterations: 36  loss: 0.5302395\n",
      "Number of iterations: 37  loss: 0.51275694\n",
      "Number of iterations: 38  loss: 0.49600467\n",
      "Number of iterations: 39  loss: 0.4799342\n",
      "Number of iterations: 40  loss: 0.4645018\n",
      "Number of iterations: 41  loss: 0.44966844\n",
      "Number of iterations: 42  loss: 0.4354\n",
      "Number of iterations: 43  loss: 0.42166528\n",
      "Number of iterations: 44  loss: 0.40843683\n",
      "Number of iterations: 45  loss: 0.39568967\n",
      "Number of iterations: 46  loss: 0.3834017\n",
      "Number of iterations: 47  loss: 0.37155262\n",
      "Number of iterations: 48  loss: 0.3601239\n",
      "Number of iterations: 49  loss: 0.34909883\n",
      "Number of iterations: 50  loss: 0.3384616\n",
      "Number of iterations: 51  loss: 0.32819793\n",
      "Number of iterations: 52  loss: 0.31829438\n",
      "Number of iterations: 53  loss: 0.3087379\n",
      "Number of iterations: 54  loss: 0.29951674\n",
      "Number of iterations: 55  loss: 0.29061955\n",
      "Number of iterations: 56  loss: 0.28203526\n",
      "Number of iterations: 57  loss: 0.2737535\n",
      "Number of iterations: 58  loss: 0.26576415\n",
      "Number of iterations: 59  loss: 0.25805762\n",
      "Number of iterations: 60  loss: 0.25062457\n",
      "Number of iterations: 61  loss: 0.24345584\n",
      "Number of iterations: 62  loss: 0.23654294\n",
      "Number of iterations: 63  loss: 0.2298771\n",
      "Number of iterations: 64  loss: 0.22344996\n",
      "Number of iterations: 65  loss: 0.21725337\n",
      "Number of iterations: 66  loss: 0.2112798\n",
      "Number of iterations: 67  loss: 0.20552093\n",
      "Number of iterations: 68  loss: 0.19996984\n",
      "Number of iterations: 69  loss: 0.19461864\n",
      "Number of iterations: 70  loss: 0.1894603\n",
      "Number of iterations: 71  loss: 0.18448783\n",
      "Number of iterations: 72  loss: 0.17969434\n",
      "Number of iterations: 73  loss: 0.17507324\n",
      "Number of iterations: 74  loss: 0.17061815\n",
      "Number of iterations: 75  loss: 0.16632253\n",
      "Number of iterations: 76  loss: 0.16218032\n",
      "Number of iterations: 77  loss: 0.15818562\n",
      "Number of iterations: 78  loss: 0.15433264\n",
      "Number of iterations: 79  loss: 0.15061572\n",
      "Number of iterations: 80  loss: 0.14702946\n",
      "Number of iterations: 81  loss: 0.14356858\n",
      "Number of iterations: 82  loss: 0.140228\n",
      "Number of iterations: 83  loss: 0.13700272\n",
      "Number of iterations: 84  loss: 0.13388808\n",
      "Number of iterations: 85  loss: 0.13087948\n",
      "Number of iterations: 86  loss: 0.12797245\n",
      "Number of iterations: 87  loss: 0.12516287\n",
      "Number of iterations: 88  loss: 0.12244652\n",
      "Number of iterations: 89  loss: 0.119819656\n",
      "Number of iterations: 90  loss: 0.117278434\n",
      "Number of iterations: 91  loss: 0.11481936\n",
      "Number of iterations: 92  loss: 0.11243915\n",
      "Number of iterations: 93  loss: 0.11013428\n",
      "Number of iterations: 94  loss: 0.107901834\n",
      "Number of iterations: 95  loss: 0.105738945\n",
      "Number of iterations: 96  loss: 0.103642724\n",
      "Number of iterations: 97  loss: 0.10161063\n",
      "Number of iterations: 98  loss: 0.09963992\n",
      "Number of iterations: 99  loss: 0.09772854\n",
      "Number of iterations: 100  loss: 0.09587393\n",
      "Number of iterations: 101  loss: 0.09407404\n",
      "Number of iterations: 102  loss: 0.09232684\n",
      "Number of iterations: 103  loss: 0.09063048\n",
      "Number of iterations: 104  loss: 0.08898293\n",
      "Number of iterations: 105  loss: 0.08738261\n",
      "Number of iterations: 106  loss: 0.0858278\n",
      "Number of iterations: 107  loss: 0.0843168\n",
      "Number of iterations: 108  loss: 0.08284833\n",
      "Number of iterations: 109  loss: 0.08142071\n",
      "Number of iterations: 110  loss: 0.08003269\n",
      "Number of iterations: 111  loss: 0.07868298\n",
      "Number of iterations: 112  loss: 0.07737037\n",
      "Number of iterations: 113  loss: 0.07609353\n",
      "Number of iterations: 114  loss: 0.07485137\n",
      "Number of iterations: 115  loss: 0.07364273\n",
      "Number of iterations: 116  loss: 0.072466634\n",
      "Number of iterations: 117  loss: 0.071321934\n",
      "Number of iterations: 118  loss: 0.07020782\n",
      "Number of iterations: 119  loss: 0.06912324\n",
      "Number of iterations: 120  loss: 0.06806732\n",
      "Number of iterations: 121  loss: 0.06703908\n",
      "Number of iterations: 122  loss: 0.06603781\n",
      "Number of iterations: 123  loss: 0.065062515\n",
      "Number of iterations: 124  loss: 0.064112514\n",
      "Number of iterations: 125  loss: 0.06318693\n",
      "Number of iterations: 126  loss: 0.062285073\n",
      "Number of iterations: 127  loss: 0.06140617\n",
      "Number of iterations: 128  loss: 0.06054953\n",
      "Number of iterations: 129  loss: 0.05971449\n",
      "Number of iterations: 130  loss: 0.058900323\n",
      "Number of iterations: 131  loss: 0.058106404\n",
      "Number of iterations: 132  loss: 0.05733215\n",
      "Number of iterations: 133  loss: 0.056576837\n",
      "Number of iterations: 134  loss: 0.055839956\n",
      "Number of iterations: 135  loss: 0.055120938\n",
      "Number of iterations: 136  loss: 0.054419175\n",
      "Number of iterations: 137  loss: 0.053734187\n",
      "Number of iterations: 138  loss: 0.053065322\n",
      "Number of iterations: 139  loss: 0.052412193\n",
      "Number of iterations: 140  loss: 0.051774297\n",
      "Number of iterations: 141  loss: 0.05115112\n",
      "Number of iterations: 142  loss: 0.050542172\n",
      "Number of iterations: 143  loss: 0.049947057\n",
      "Number of iterations: 144  loss: 0.049365338\n",
      "Number of iterations: 145  loss: 0.04879652\n",
      "Number of iterations: 146  loss: 0.04824028\n",
      "Number of iterations: 147  loss: 0.04769618\n",
      "Number of iterations: 148  loss: 0.047163915\n",
      "Number of iterations: 149  loss: 0.04664302\n",
      "Number of iterations: 150  loss: 0.046133157\n",
      "Number of iterations: 151  loss: 0.045634095\n",
      "Number of iterations: 152  loss: 0.04514535\n",
      "Number of iterations: 153  loss: 0.04466671\n",
      "Number of iterations: 154  loss: 0.04419782\n",
      "Number of iterations: 155  loss: 0.0437384\n",
      "Number of iterations: 156  loss: 0.04328812\n",
      "Number of iterations: 157  loss: 0.042846784\n",
      "Number of iterations: 158  loss: 0.04241406\n",
      "Number of iterations: 159  loss: 0.041989733\n",
      "Number of iterations: 160  loss: 0.041573547\n",
      "Number of iterations: 161  loss: 0.041165244\n",
      "Number of iterations: 162  loss: 0.040764585\n",
      "Number of iterations: 163  loss: 0.040371377\n",
      "Number of iterations: 164  loss: 0.039985377\n",
      "Number of iterations: 165  loss: 0.03960642\n",
      "Number of iterations: 166  loss: 0.039234266\n",
      "Number of iterations: 167  loss: 0.038868733\n",
      "Number of iterations: 168  loss: 0.03850967\n",
      "Number of iterations: 169  loss: 0.0381568\n",
      "Number of iterations: 170  loss: 0.037810076\n",
      "Number of iterations: 171  loss: 0.037469253\n",
      "Number of iterations: 172  loss: 0.03713422\n",
      "Number of iterations: 173  loss: 0.036804758\n",
      "Number of iterations: 174  loss: 0.03648075\n",
      "Number of iterations: 175  loss: 0.036162075\n",
      "Number of iterations: 176  loss: 0.03584857\n",
      "Number of iterations: 177  loss: 0.035540145\n",
      "Number of iterations: 178  loss: 0.03523658\n",
      "Number of iterations: 179  loss: 0.03493778\n",
      "Number of iterations: 180  loss: 0.0346436\n",
      "Number of iterations: 181  loss: 0.034354046\n",
      "Number of iterations: 182  loss: 0.034068894\n",
      "Number of iterations: 183  loss: 0.033788096\n",
      "Number of iterations: 184  loss: 0.033511516\n",
      "Number of iterations: 185  loss: 0.033239\n",
      "Number of iterations: 186  loss: 0.03297051\n",
      "Number of iterations: 187  loss: 0.032705955\n",
      "Number of iterations: 188  loss: 0.03244527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 189  loss: 0.03218828\n",
      "Number of iterations: 190  loss: 0.031934973\n",
      "Number of iterations: 191  loss: 0.031685207\n",
      "Number of iterations: 192  loss: 0.03143895\n",
      "Number of iterations: 193  loss: 0.031196093\n",
      "Number of iterations: 194  loss: 0.03095657\n",
      "Number of iterations: 195  loss: 0.03072035\n",
      "Number of iterations: 196  loss: 0.030487288\n",
      "Number of iterations: 197  loss: 0.030257335\n",
      "Number of iterations: 198  loss: 0.03003045\n",
      "Number of iterations: 199  loss: 0.029806562\n",
      "Number of iterations: 200  loss: 0.029585566\n",
      "Number of iterations: 201  loss: 0.029367495\n",
      "Number of iterations: 202  loss: 0.029152237\n",
      "Number of iterations: 203  loss: 0.028939681\n",
      "Number of iterations: 204  loss: 0.0287298\n",
      "Number of iterations: 205  loss: 0.028522613\n",
      "Number of iterations: 206  loss: 0.028318007\n",
      "Number of iterations: 207  loss: 0.02811594\n",
      "Number of iterations: 208  loss: 0.027916348\n",
      "Number of iterations: 209  loss: 0.027719162\n",
      "Number of iterations: 210  loss: 0.02752438\n",
      "Number of iterations: 211  loss: 0.02733198\n",
      "Number of iterations: 212  loss: 0.027141858\n",
      "Number of iterations: 213  loss: 0.026953975\n",
      "Number of iterations: 214  loss: 0.026768316\n",
      "Number of iterations: 215  loss: 0.026584802\n",
      "Number of iterations: 216  loss: 0.026403438\n",
      "Number of iterations: 217  loss: 0.026224209\n",
      "Number of iterations: 218  loss: 0.02604698\n",
      "Number of iterations: 219  loss: 0.025871778\n",
      "Number of iterations: 220  loss: 0.025698552\n",
      "Number of iterations: 221  loss: 0.025527272\n",
      "Number of iterations: 222  loss: 0.025357928\n",
      "Number of iterations: 223  loss: 0.025190445\n",
      "Number of iterations: 224  loss: 0.025024837\n",
      "Number of iterations: 225  loss: 0.024860991\n",
      "Number of iterations: 226  loss: 0.024698941\n",
      "Number of iterations: 227  loss: 0.024538653\n",
      "Number of iterations: 228  loss: 0.024380079\n",
      "Number of iterations: 229  loss: 0.024223192\n",
      "Number of iterations: 230  loss: 0.024067994\n",
      "Number of iterations: 231  loss: 0.023914421\n",
      "Number of iterations: 232  loss: 0.02376244\n",
      "Number of iterations: 233  loss: 0.023612034\n",
      "Number of iterations: 234  loss: 0.023463178\n",
      "Number of iterations: 235  loss: 0.023315901\n",
      "Number of iterations: 236  loss: 0.02317011\n",
      "Number of iterations: 237  loss: 0.023025773\n",
      "Number of iterations: 238  loss: 0.022882875\n",
      "Number of iterations: 239  loss: 0.022741463\n",
      "Number of iterations: 240  loss: 0.02260143\n",
      "Number of iterations: 241  loss: 0.022462806\n",
      "Number of iterations: 242  loss: 0.022325527\n",
      "Number of iterations: 243  loss: 0.022189597\n",
      "Number of iterations: 244  loss: 0.02205497\n",
      "Number of iterations: 245  loss: 0.021921668\n",
      "Number of iterations: 246  loss: 0.021789677\n",
      "Number of iterations: 247  loss: 0.021658905\n",
      "Number of iterations: 248  loss: 0.021529384\n",
      "Number of iterations: 249  loss: 0.0214011\n",
      "Number of iterations: 250  loss: 0.021274008\n",
      "Number of iterations: 251  loss: 0.021148102\n",
      "Number of iterations: 252  loss: 0.021023382\n",
      "Number of iterations: 253  loss: 0.020899797\n",
      "Number of iterations: 254  loss: 0.020777358\n",
      "Number of iterations: 255  loss: 0.02065601\n",
      "Number of iterations: 256  loss: 0.020535791\n",
      "Number of iterations: 257  loss: 0.020416657\n",
      "Number of iterations: 258  loss: 0.020298582\n",
      "Number of iterations: 259  loss: 0.020181565\n",
      "Number of iterations: 260  loss: 0.020065585\n",
      "Number of iterations: 261  loss: 0.019950612\n",
      "Number of iterations: 262  loss: 0.019836696\n",
      "Number of iterations: 263  loss: 0.019723734\n",
      "Number of iterations: 264  loss: 0.019611765\n",
      "Number of iterations: 265  loss: 0.019500725\n",
      "Number of iterations: 266  loss: 0.01939066\n",
      "Number of iterations: 267  loss: 0.019281555\n",
      "Number of iterations: 268  loss: 0.019173367\n",
      "Number of iterations: 269  loss: 0.019066066\n",
      "Number of iterations: 270  loss: 0.018959666\n",
      "Number of iterations: 271  loss: 0.018854173\n",
      "Number of iterations: 272  loss: 0.018749539\n",
      "Number of iterations: 273  loss: 0.018645756\n",
      "Number of iterations: 274  loss: 0.018542834\n",
      "Number of iterations: 275  loss: 0.018440763\n",
      "Number of iterations: 276  loss: 0.018339513\n",
      "Number of iterations: 277  loss: 0.018239064\n",
      "Number of iterations: 278  loss: 0.01813943\n",
      "Number of iterations: 279  loss: 0.01804059\n",
      "Number of iterations: 280  loss: 0.017942509\n",
      "Number of iterations: 281  loss: 0.017845193\n",
      "Number of iterations: 282  loss: 0.017748678\n",
      "Number of iterations: 283  loss: 0.01765289\n",
      "Number of iterations: 284  loss: 0.017557822\n",
      "Number of iterations: 285  loss: 0.017463509\n",
      "Number of iterations: 286  loss: 0.017369915\n",
      "Number of iterations: 287  loss: 0.017277014\n",
      "Number of iterations: 288  loss: 0.017184814\n",
      "Number of iterations: 289  loss: 0.017093321\n",
      "Number of iterations: 290  loss: 0.01700249\n",
      "Number of iterations: 291  loss: 0.016912349\n",
      "Number of iterations: 292  loss: 0.016822863\n",
      "Number of iterations: 293  loss: 0.016734043\n",
      "Number of iterations: 294  loss: 0.01664587\n",
      "Number of iterations: 295  loss: 0.016558306\n",
      "Number of iterations: 296  loss: 0.016471384\n",
      "Number of iterations: 297  loss: 0.016385082\n",
      "Number of iterations: 298  loss: 0.01629939\n",
      "Number of iterations: 299  loss: 0.016214317\n",
      "Number of iterations: 300  loss: 0.01612984\n",
      "Number of iterations: 301  loss: 0.016045943\n",
      "Number of iterations: 302  loss: 0.015962623\n",
      "Number of iterations: 303  loss: 0.01587987\n",
      "Number of iterations: 304  loss: 0.015797712\n",
      "Number of iterations: 305  loss: 0.015716083\n",
      "Number of iterations: 306  loss: 0.015635015\n",
      "Number of iterations: 307  loss: 0.0155545\n",
      "Number of iterations: 308  loss: 0.015474526\n",
      "Number of iterations: 309  loss: 0.0153950695\n",
      "Number of iterations: 310  loss: 0.015316127\n",
      "Number of iterations: 311  loss: 0.015237719\n",
      "Number of iterations: 312  loss: 0.015159811\n",
      "Number of iterations: 313  loss: 0.015082427\n",
      "Number of iterations: 314  loss: 0.015005505\n",
      "Number of iterations: 315  loss: 0.014929095\n",
      "Number of iterations: 316  loss: 0.014853192\n",
      "Number of iterations: 317  loss: 0.014777756\n",
      "Number of iterations: 318  loss: 0.014702793\n",
      "Number of iterations: 319  loss: 0.014628285\n",
      "Number of iterations: 320  loss: 0.014554248\n",
      "Number of iterations: 321  loss: 0.01448068\n",
      "Number of iterations: 322  loss: 0.014407555\n",
      "Number of iterations: 323  loss: 0.014334872\n",
      "Number of iterations: 324  loss: 0.014262651\n",
      "Number of iterations: 325  loss: 0.014190849\n",
      "Number of iterations: 326  loss: 0.014119478\n",
      "Number of iterations: 327  loss: 0.014048536\n",
      "Number of iterations: 328  loss: 0.013978004\n",
      "Number of iterations: 329  loss: 0.013907911\n",
      "Number of iterations: 330  loss: 0.013838219\n",
      "Number of iterations: 331  loss: 0.013768921\n",
      "Number of iterations: 332  loss: 0.01370004\n",
      "Number of iterations: 333  loss: 0.013631555\n",
      "Number of iterations: 334  loss: 0.013563478\n",
      "Number of iterations: 335  loss: 0.013495775\n",
      "Number of iterations: 336  loss: 0.013428462\n",
      "Number of iterations: 337  loss: 0.013361518\n",
      "Number of iterations: 338  loss: 0.013294949\n",
      "Number of iterations: 339  loss: 0.013228765\n",
      "Number of iterations: 340  loss: 0.013162956\n",
      "Number of iterations: 341  loss: 0.013097501\n",
      "Number of iterations: 342  loss: 0.013032415\n",
      "Number of iterations: 343  loss: 0.012967686\n",
      "Number of iterations: 344  loss: 0.012903305\n",
      "Number of iterations: 345  loss: 0.012839282\n",
      "Number of iterations: 346  loss: 0.012775603\n",
      "Number of iterations: 347  loss: 0.012712283\n",
      "Number of iterations: 348  loss: 0.012649292\n",
      "Number of iterations: 349  loss: 0.0125866365\n",
      "Number of iterations: 350  loss: 0.012524315\n",
      "Number of iterations: 351  loss: 0.012462348\n",
      "Number of iterations: 352  loss: 0.0124006905\n",
      "Number of iterations: 353  loss: 0.012339364\n",
      "Number of iterations: 354  loss: 0.012278362\n",
      "Number of iterations: 355  loss: 0.012217682\n",
      "Number of iterations: 356  loss: 0.012157311\n",
      "Number of iterations: 357  loss: 0.012097267\n",
      "Number of iterations: 358  loss: 0.01203753\n",
      "Number of iterations: 359  loss: 0.011978111\n",
      "Number of iterations: 360  loss: 0.011918981\n",
      "Number of iterations: 361  loss: 0.011860173\n",
      "Number of iterations: 362  loss: 0.01180166\n",
      "Number of iterations: 363  loss: 0.011743458\n",
      "Number of iterations: 364  loss: 0.01168555\n",
      "Number of iterations: 365  loss: 0.011627952\n",
      "Number of iterations: 366  loss: 0.0115706315\n",
      "Number of iterations: 367  loss: 0.011513603\n",
      "Number of iterations: 368  loss: 0.011456865\n",
      "Number of iterations: 369  loss: 0.011400414\n",
      "Number of iterations: 370  loss: 0.011344258\n",
      "Number of iterations: 371  loss: 0.011288374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 372  loss: 0.011232777\n",
      "Number of iterations: 373  loss: 0.0111774625\n",
      "Number of iterations: 374  loss: 0.011122429\n",
      "Number of iterations: 375  loss: 0.011067678\n",
      "Number of iterations: 376  loss: 0.011013182\n",
      "Number of iterations: 377  loss: 0.010958985\n",
      "Number of iterations: 378  loss: 0.0109050395\n",
      "Number of iterations: 379  loss: 0.010851367\n",
      "Number of iterations: 380  loss: 0.010797977\n",
      "Number of iterations: 381  loss: 0.0107448455\n",
      "Number of iterations: 382  loss: 0.010691979\n",
      "Number of iterations: 383  loss: 0.010639368\n",
      "Number of iterations: 384  loss: 0.010587017\n",
      "Number of iterations: 385  loss: 0.010534953\n",
      "Number of iterations: 386  loss: 0.010483128\n",
      "Number of iterations: 387  loss: 0.010431566\n",
      "Number of iterations: 388  loss: 0.010380264\n",
      "Number of iterations: 389  loss: 0.010329211\n",
      "Number of iterations: 390  loss: 0.01027842\n",
      "Number of iterations: 391  loss: 0.010227879\n",
      "Number of iterations: 392  loss: 0.010177588\n",
      "Number of iterations: 393  loss: 0.01012755\n",
      "Number of iterations: 394  loss: 0.010077764\n",
      "Number of iterations: 395  loss: 0.010028228\n",
      "Number of iterations: 396  loss: 0.009978942\n",
      "Number of iterations: 397  loss: 0.009929885\n",
      "Number of iterations: 398  loss: 0.009881071\n",
      "Number of iterations: 399  loss: 0.009832526\n",
      "Number of iterations: 400  loss: 0.009784209\n",
      "Number of iterations: 401  loss: 0.009736133\n",
      "Number of iterations: 402  loss: 0.009688304\n",
      "Number of iterations: 403  loss: 0.009640702\n",
      "Number of iterations: 404  loss: 0.009593373\n",
      "Number of iterations: 405  loss: 0.009546246\n",
      "Number of iterations: 406  loss: 0.0094993645\n",
      "Number of iterations: 407  loss: 0.009452726\n",
      "Number of iterations: 408  loss: 0.00940632\n",
      "Number of iterations: 409  loss: 0.009360163\n",
      "Number of iterations: 410  loss: 0.009314218\n",
      "Number of iterations: 411  loss: 0.009268519\n",
      "Number of iterations: 412  loss: 0.009223032\n",
      "Number of iterations: 413  loss: 0.009177798\n",
      "Number of iterations: 414  loss: 0.0091327755\n",
      "Number of iterations: 415  loss: 0.009087981\n",
      "Number of iterations: 416  loss: 0.009043417\n",
      "Number of iterations: 417  loss: 0.008999112\n",
      "Number of iterations: 418  loss: 0.00895498\n",
      "Number of iterations: 419  loss: 0.008911099\n",
      "Number of iterations: 420  loss: 0.008867449\n",
      "Number of iterations: 421  loss: 0.008824029\n",
      "Number of iterations: 422  loss: 0.008780813\n",
      "Number of iterations: 423  loss: 0.00873783\n",
      "Number of iterations: 424  loss: 0.008695075\n",
      "Number of iterations: 425  loss: 0.008652526\n",
      "Number of iterations: 426  loss: 0.008610213\n",
      "Number of iterations: 427  loss: 0.008568094\n",
      "Number of iterations: 428  loss: 0.008526213\n",
      "Number of iterations: 429  loss: 0.0084845545\n",
      "Number of iterations: 430  loss: 0.008443089\n",
      "Number of iterations: 431  loss: 0.0084018605\n",
      "Number of iterations: 432  loss: 0.008360847\n",
      "Number of iterations: 433  loss: 0.008320038\n",
      "Number of iterations: 434  loss: 0.008279467\n",
      "Number of iterations: 435  loss: 0.008239085\n",
      "Number of iterations: 436  loss: 0.008198926\n",
      "Number of iterations: 437  loss: 0.008158975\n",
      "Number of iterations: 438  loss: 0.008119243\n",
      "Number of iterations: 439  loss: 0.008079719\n",
      "Number of iterations: 440  loss: 0.008040402\n",
      "Number of iterations: 441  loss: 0.008001297\n",
      "Number of iterations: 442  loss: 0.007962395\n",
      "Number of iterations: 443  loss: 0.00792371\n",
      "Number of iterations: 444  loss: 0.007885232\n",
      "Number of iterations: 445  loss: 0.007846951\n",
      "Number of iterations: 446  loss: 0.0078088962\n",
      "Number of iterations: 447  loss: 0.007771026\n",
      "Number of iterations: 448  loss: 0.0077333697\n",
      "Number of iterations: 449  loss: 0.0076959142\n",
      "Number of iterations: 450  loss: 0.007658671\n",
      "Number of iterations: 451  loss: 0.007621634\n",
      "Number of iterations: 452  loss: 0.0075847823\n",
      "Number of iterations: 453  loss: 0.0075481483\n",
      "Number of iterations: 454  loss: 0.0075117094\n",
      "Number of iterations: 455  loss: 0.007475476\n",
      "Number of iterations: 456  loss: 0.0074394387\n",
      "Number of iterations: 457  loss: 0.007403602\n",
      "Number of iterations: 458  loss: 0.0073679555\n",
      "Number of iterations: 459  loss: 0.0073325094\n",
      "Number of iterations: 460  loss: 0.0072972733\n",
      "Number of iterations: 461  loss: 0.0072622155\n",
      "Number of iterations: 462  loss: 0.00722736\n",
      "Number of iterations: 463  loss: 0.0071927044\n",
      "Number of iterations: 464  loss: 0.007158243\n",
      "Number of iterations: 465  loss: 0.007123989\n",
      "Number of iterations: 466  loss: 0.007089902\n",
      "Number of iterations: 467  loss: 0.007056035\n",
      "Number of iterations: 468  loss: 0.0070223366\n",
      "Number of iterations: 469  loss: 0.006988849\n",
      "Number of iterations: 470  loss: 0.006955536\n",
      "Number of iterations: 471  loss: 0.0069224327\n",
      "Number of iterations: 472  loss: 0.006889504\n",
      "Number of iterations: 473  loss: 0.006856759\n",
      "Number of iterations: 474  loss: 0.006824223\n",
      "Number of iterations: 475  loss: 0.0067918566\n",
      "Number of iterations: 476  loss: 0.0067596934\n",
      "Number of iterations: 477  loss: 0.0067276973\n",
      "Number of iterations: 478  loss: 0.006695894\n",
      "Number of iterations: 479  loss: 0.006664287\n",
      "Number of iterations: 480  loss: 0.006632856\n",
      "Number of iterations: 481  loss: 0.0066016093\n",
      "Number of iterations: 482  loss: 0.0065705464\n",
      "Number of iterations: 483  loss: 0.0065396684\n",
      "Number of iterations: 484  loss: 0.0065089683\n",
      "Number of iterations: 485  loss: 0.00647845\n",
      "Number of iterations: 486  loss: 0.006448123\n",
      "Number of iterations: 487  loss: 0.0064179534\n",
      "Number of iterations: 488  loss: 0.0063879923\n",
      "Number of iterations: 489  loss: 0.0063581844\n",
      "Number of iterations: 490  loss: 0.0063285623\n",
      "Number of iterations: 491  loss: 0.006299134\n",
      "Number of iterations: 492  loss: 0.0062698703\n",
      "Number of iterations: 493  loss: 0.0062407753\n",
      "Number of iterations: 494  loss: 0.006211869\n",
      "Number of iterations: 495  loss: 0.0061831186\n",
      "Number of iterations: 496  loss: 0.0061545596\n",
      "Number of iterations: 497  loss: 0.006126172\n",
      "Number of iterations: 498  loss: 0.0060979603\n",
      "Number of iterations: 499  loss: 0.0060698944\n",
      "Number of iterations: 500  loss: 0.0060420414\n",
      "Number of iterations: 501  loss: 0.0060143215\n",
      "Number of iterations: 502  loss: 0.0059867795\n",
      "Number of iterations: 503  loss: 0.005959412\n",
      "Number of iterations: 504  loss: 0.0059322184\n",
      "Number of iterations: 505  loss: 0.0059051896\n",
      "Number of iterations: 506  loss: 0.0058783153\n",
      "Number of iterations: 507  loss: 0.00585161\n",
      "Number of iterations: 508  loss: 0.0058250884\n",
      "Number of iterations: 509  loss: 0.0057987263\n",
      "Number of iterations: 510  loss: 0.0057725064\n",
      "Number of iterations: 511  loss: 0.0057464787\n",
      "Number of iterations: 512  loss: 0.0057206\n",
      "Number of iterations: 513  loss: 0.005694879\n",
      "Number of iterations: 514  loss: 0.00566933\n",
      "Number of iterations: 515  loss: 0.005643937\n",
      "Number of iterations: 516  loss: 0.005618693\n",
      "Number of iterations: 517  loss: 0.0055936296\n",
      "Number of iterations: 518  loss: 0.0055686985\n",
      "Number of iterations: 519  loss: 0.0055439495\n",
      "Number of iterations: 520  loss: 0.005519351\n",
      "Number of iterations: 521  loss: 0.0054948973\n",
      "Number of iterations: 522  loss: 0.005470623\n",
      "Number of iterations: 523  loss: 0.0054464745\n",
      "Number of iterations: 524  loss: 0.0054225\n",
      "Number of iterations: 525  loss: 0.0053986707\n",
      "Number of iterations: 526  loss: 0.005375\n",
      "Number of iterations: 527  loss: 0.005351471\n",
      "Number of iterations: 528  loss: 0.005328103\n",
      "Number of iterations: 529  loss: 0.005304888\n",
      "Number of iterations: 530  loss: 0.005281819\n",
      "Number of iterations: 531  loss: 0.005258888\n",
      "Number of iterations: 532  loss: 0.005236127\n",
      "Number of iterations: 533  loss: 0.005213502\n",
      "Number of iterations: 534  loss: 0.0051910123\n",
      "Number of iterations: 535  loss: 0.005168688\n",
      "Number of iterations: 536  loss: 0.0051465025\n",
      "Number of iterations: 537  loss: 0.0051244604\n",
      "Number of iterations: 538  loss: 0.005102564\n",
      "Number of iterations: 539  loss: 0.0050808\n",
      "Number of iterations: 540  loss: 0.005059197\n",
      "Number of iterations: 541  loss: 0.0050377254\n",
      "Number of iterations: 542  loss: 0.0050163935\n",
      "Number of iterations: 543  loss: 0.004995211\n",
      "Number of iterations: 544  loss: 0.00497417\n",
      "Number of iterations: 545  loss: 0.004953258\n",
      "Number of iterations: 546  loss: 0.004932479\n",
      "Number of iterations: 547  loss: 0.0049118698\n",
      "Number of iterations: 548  loss: 0.0048913597\n",
      "Number of iterations: 549  loss: 0.004871012\n",
      "Number of iterations: 550  loss: 0.0048507783\n",
      "Number of iterations: 551  loss: 0.004830704\n",
      "Number of iterations: 552  loss: 0.0048107435\n",
      "Number of iterations: 553  loss: 0.0047909347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 554  loss: 0.0047712354\n",
      "Number of iterations: 555  loss: 0.0047517023\n",
      "Number of iterations: 556  loss: 0.0047322614\n",
      "Number of iterations: 557  loss: 0.004713\n",
      "Number of iterations: 558  loss: 0.0046938057\n",
      "Number of iterations: 559  loss: 0.0046748198\n",
      "Number of iterations: 560  loss: 0.0046558697\n",
      "Number of iterations: 561  loss: 0.004637144\n",
      "Number of iterations: 562  loss: 0.004618451\n",
      "Number of iterations: 563  loss: 0.0045999903\n",
      "Number of iterations: 564  loss: 0.0045815357\n",
      "Number of iterations: 565  loss: 0.004563354\n",
      "Number of iterations: 566  loss: 0.004545132\n",
      "Number of iterations: 567  loss: 0.004527198\n",
      "Number of iterations: 568  loss: 0.004509205\n",
      "Number of iterations: 569  loss: 0.0044915536\n",
      "Number of iterations: 570  loss: 0.0044737854\n",
      "Number of iterations: 571  loss: 0.0044563967\n",
      "Number of iterations: 572  loss: 0.0044388394\n",
      "Number of iterations: 573  loss: 0.0044217384\n",
      "Number of iterations: 574  loss: 0.004404372\n",
      "Number of iterations: 575  loss: 0.004387564\n",
      "Number of iterations: 576  loss: 0.004370359\n",
      "Number of iterations: 577  loss: 0.004353887\n",
      "Number of iterations: 578  loss: 0.004336797\n",
      "Number of iterations: 579  loss: 0.004320709\n",
      "Number of iterations: 580  loss: 0.0043036784\n",
      "Number of iterations: 581  loss: 0.004288036\n",
      "Number of iterations: 582  loss: 0.0042709406\n",
      "Number of iterations: 583  loss: 0.0042559137\n",
      "Number of iterations: 584  loss: 0.0042385566\n",
      "Number of iterations: 585  loss: 0.0042243963\n",
      "Number of iterations: 586  loss: 0.004206419\n",
      "Number of iterations: 587  loss: 0.004193602\n",
      "Number of iterations: 588  loss: 0.004174353\n",
      "Number of iterations: 589  loss: 0.004163803\n",
      "Number of iterations: 590  loss: 0.0041420353\n",
      "Number of iterations: 591  loss: 0.0041355104\n",
      "Number of iterations: 592  loss: 0.0041088536\n",
      "Number of iterations: 593  loss: 0.0041098315\n",
      "Number of iterations: 594  loss: 0.0040737046\n",
      "Number of iterations: 595  loss: 0.0040893694\n",
      "Number of iterations: 596  loss: 0.0040347786\n",
      "Number of iterations: 597  loss: 0.0040808283\n",
      "Number of iterations: 598  loss: 0.00399055\n",
      "Number of iterations: 599  loss: 0.004103909\n",
      "Number of iterations: 600  loss: 0.003948554\n",
      "Number of iterations: 601  loss: 0.0042264075\n",
      "Number of iterations: 602  loss: 0.0039774203\n",
      "Number of iterations: 603  loss: 0.004718735\n",
      "Number of iterations: 604  loss: 0.004484053\n",
      "Number of iterations: 605  loss: 0.006758076\n",
      "Number of iterations: 606  loss: 0.0074643227\n",
      "Number of iterations: 607  loss: 0.014865283\n",
      "Number of iterations: 608  loss: 0.01850423\n",
      "Number of iterations: 609  loss: 0.03307188\n",
      "Number of iterations: 610  loss: 0.028277157\n",
      "Number of iterations: 611  loss: 0.029198619\n",
      "Number of iterations: 612  loss: 0.012737829\n",
      "Number of iterations: 613  loss: 0.009456529\n",
      "Number of iterations: 614  loss: 0.0048281453\n",
      "Number of iterations: 615  loss: 0.0049076797\n",
      "Number of iterations: 616  loss: 0.0041546146\n",
      "Number of iterations: 617  loss: 0.0043169437\n",
      "Number of iterations: 618  loss: 0.004072662\n",
      "Number of iterations: 619  loss: 0.0040954323\n",
      "Number of iterations: 620  loss: 0.004029847\n",
      "Number of iterations: 621  loss: 0.003976676\n",
      "Number of iterations: 622  loss: 0.003972285\n",
      "Number of iterations: 623  loss: 0.0039063077\n",
      "Number of iterations: 624  loss: 0.003901345\n",
      "Number of iterations: 625  loss: 0.0038546955\n",
      "Number of iterations: 626  loss: 0.0038383172\n",
      "Number of iterations: 627  loss: 0.0038072073\n",
      "Number of iterations: 628  loss: 0.003786989\n",
      "Number of iterations: 629  loss: 0.003762189\n",
      "Number of iterations: 630  loss: 0.003742716\n",
      "Number of iterations: 631  loss: 0.0037210886\n",
      "Number of iterations: 632  loss: 0.0037029579\n",
      "Number of iterations: 633  loss: 0.0036836853\n",
      "Number of iterations: 634  loss: 0.0036667818\n",
      "Number of iterations: 635  loss: 0.0036493626\n",
      "Number of iterations: 636  loss: 0.0036335837\n",
      "Number of iterations: 637  loss: 0.0036175882\n",
      "Number of iterations: 638  loss: 0.003602854\n",
      "Number of iterations: 639  loss: 0.0035880113\n",
      "Number of iterations: 640  loss: 0.0035741825\n",
      "Number of iterations: 641  loss: 0.0035602872\n",
      "Number of iterations: 642  loss: 0.0035472463\n",
      "Number of iterations: 643  loss: 0.0035341482\n",
      "Number of iterations: 644  loss: 0.0035217986\n",
      "Number of iterations: 645  loss: 0.003509373\n",
      "Number of iterations: 646  loss: 0.0034976364\n",
      "Number of iterations: 647  loss: 0.0034857637\n",
      "Number of iterations: 648  loss: 0.0034745815\n",
      "Number of iterations: 649  loss: 0.0034631991\n",
      "Number of iterations: 650  loss: 0.0034524973\n",
      "Number of iterations: 651  loss: 0.003441527\n",
      "Number of iterations: 652  loss: 0.0034312655\n",
      "Number of iterations: 653  loss: 0.003420646\n",
      "Number of iterations: 654  loss: 0.0034108039\n",
      "Number of iterations: 655  loss: 0.0034004552\n",
      "Number of iterations: 656  loss: 0.0033910272\n",
      "Number of iterations: 657  loss: 0.003380882\n",
      "Number of iterations: 658  loss: 0.0033718762\n",
      "Number of iterations: 659  loss: 0.0033618389\n",
      "Number of iterations: 660  loss: 0.0033533105\n",
      "Number of iterations: 661  loss: 0.0033432648\n",
      "Number of iterations: 662  loss: 0.0033352934\n",
      "Number of iterations: 663  loss: 0.00332508\n",
      "Number of iterations: 664  loss: 0.0033178427\n",
      "Number of iterations: 665  loss: 0.0033071884\n",
      "Number of iterations: 666  loss: 0.0033009846\n",
      "Number of iterations: 667  loss: 0.0032894467\n",
      "Number of iterations: 668  loss: 0.0032848443\n",
      "Number of iterations: 669  loss: 0.0032716363\n",
      "Number of iterations: 670  loss: 0.0032696784\n",
      "Number of iterations: 671  loss: 0.0032534103\n",
      "Number of iterations: 672  loss: 0.0032560327\n",
      "Number of iterations: 673  loss: 0.0032341317\n",
      "Number of iterations: 674  loss: 0.0032451218\n",
      "Number of iterations: 675  loss: 0.0032127416\n",
      "Number of iterations: 676  loss: 0.0032397548\n",
      "Number of iterations: 677  loss: 0.003187817\n",
      "Number of iterations: 678  loss: 0.003247115\n",
      "Number of iterations: 679  loss: 0.0031591365\n",
      "Number of iterations: 680  loss: 0.0032882094\n",
      "Number of iterations: 681  loss: 0.0031382048\n",
      "Number of iterations: 682  loss: 0.0034339095\n",
      "Number of iterations: 683  loss: 0.0032035185\n",
      "Number of iterations: 684  loss: 0.0039570737\n",
      "Number of iterations: 685  loss: 0.0037734017\n",
      "Number of iterations: 686  loss: 0.0059996964\n",
      "Number of iterations: 687  loss: 0.006802996\n",
      "Number of iterations: 688  loss: 0.013985525\n",
      "Number of iterations: 689  loss: 0.018344592\n",
      "Number of iterations: 690  loss: 0.034127083\n",
      "Number of iterations: 691  loss: 0.03199905\n",
      "Number of iterations: 692  loss: 0.032639276\n",
      "Number of iterations: 693  loss: 0.013870428\n",
      "Number of iterations: 694  loss: 0.008218026\n",
      "Number of iterations: 695  loss: 0.003827398\n",
      "Number of iterations: 696  loss: 0.0037560898\n",
      "Number of iterations: 697  loss: 0.0035995443\n",
      "Number of iterations: 698  loss: 0.0035147567\n",
      "Number of iterations: 699  loss: 0.0035013815\n",
      "Number of iterations: 700  loss: 0.0034200975\n",
      "Number of iterations: 701  loss: 0.0034102509\n",
      "Number of iterations: 702  loss: 0.0033401405\n",
      "Number of iterations: 703  loss: 0.0033356682\n",
      "Number of iterations: 704  loss: 0.0032813028\n",
      "Number of iterations: 705  loss: 0.003266305\n",
      "Number of iterations: 706  loss: 0.0032356791\n",
      "Number of iterations: 707  loss: 0.0032114438\n",
      "Number of iterations: 708  loss: 0.0031923766\n",
      "Number of iterations: 709  loss: 0.003168999\n",
      "Number of iterations: 710  loss: 0.0031520522\n",
      "Number of iterations: 711  loss: 0.0031328846\n",
      "Number of iterations: 712  loss: 0.0031167746\n",
      "Number of iterations: 713  loss: 0.0031005854\n",
      "Number of iterations: 714  loss: 0.0030858407\n",
      "Number of iterations: 715  loss: 0.0030715638\n",
      "Number of iterations: 716  loss: 0.0030582552\n",
      "Number of iterations: 717  loss: 0.0030454253\n",
      "Number of iterations: 718  loss: 0.0030333202\n",
      "Number of iterations: 719  loss: 0.0030216859\n",
      "Number of iterations: 720  loss: 0.0030106104\n",
      "Number of iterations: 721  loss: 0.0029999637\n",
      "Number of iterations: 722  loss: 0.0029897504\n",
      "Number of iterations: 723  loss: 0.002979929\n",
      "Number of iterations: 724  loss: 0.0029704636\n",
      "Number of iterations: 725  loss: 0.0029613236\n",
      "Number of iterations: 726  loss: 0.0029524867\n",
      "Number of iterations: 727  loss: 0.002943945\n",
      "Number of iterations: 728  loss: 0.0029356629\n",
      "Number of iterations: 729  loss: 0.0029276216\n",
      "Number of iterations: 730  loss: 0.0029198178\n",
      "Number of iterations: 731  loss: 0.0029122196\n",
      "Number of iterations: 732  loss: 0.0029048224\n",
      "Number of iterations: 733  loss: 0.0028976062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 734  loss: 0.0028905615\n",
      "Number of iterations: 735  loss: 0.0028836925\n",
      "Number of iterations: 736  loss: 0.002876963\n",
      "Number of iterations: 737  loss: 0.0028703874\n",
      "Number of iterations: 738  loss: 0.002863951\n",
      "Number of iterations: 739  loss: 0.0028576308\n",
      "Number of iterations: 740  loss: 0.002851431\n",
      "Number of iterations: 741  loss: 0.0028453523\n",
      "Number of iterations: 742  loss: 0.0028393716\n",
      "Number of iterations: 743  loss: 0.00283351\n",
      "Number of iterations: 744  loss: 0.0028277237\n",
      "Number of iterations: 745  loss: 0.002822045\n",
      "Number of iterations: 746  loss: 0.0028164438\n",
      "Number of iterations: 747  loss: 0.0028109306\n",
      "Number of iterations: 748  loss: 0.0028054863\n",
      "Number of iterations: 749  loss: 0.0028001415\n",
      "Number of iterations: 750  loss: 0.0027948276\n",
      "Number of iterations: 751  loss: 0.002789636\n",
      "Number of iterations: 752  loss: 0.0027844347\n",
      "Number of iterations: 753  loss: 0.00277939\n",
      "Number of iterations: 754  loss: 0.0027742898\n",
      "Number of iterations: 755  loss: 0.0027693808\n",
      "Number of iterations: 756  loss: 0.0027643624\n",
      "Number of iterations: 757  loss: 0.002759611\n",
      "Number of iterations: 758  loss: 0.0027546242\n",
      "Number of iterations: 759  loss: 0.0027500615\n",
      "Number of iterations: 760  loss: 0.0027450316\n",
      "Number of iterations: 761  loss: 0.00274076\n",
      "Number of iterations: 762  loss: 0.0027355503\n",
      "Number of iterations: 763  loss: 0.0027317232\n",
      "Number of iterations: 764  loss: 0.0027260717\n",
      "Number of iterations: 765  loss: 0.0027230682\n",
      "Number of iterations: 766  loss: 0.0027164423\n",
      "Number of iterations: 767  loss: 0.0027150342\n",
      "Number of iterations: 768  loss: 0.0027062914\n",
      "Number of iterations: 769  loss: 0.002708217\n",
      "Number of iterations: 770  loss: 0.0026948755\n",
      "Number of iterations: 771  loss: 0.0027041142\n",
      "Number of iterations: 772  loss: 0.002680709\n",
      "Number of iterations: 773  loss: 0.0027068951\n",
      "Number of iterations: 774  loss: 0.0026617288\n",
      "Number of iterations: 775  loss: 0.0027301647\n",
      "Number of iterations: 776  loss: 0.0026410625\n",
      "Number of iterations: 777  loss: 0.0028280471\n",
      "Number of iterations: 778  loss: 0.0026761314\n",
      "Number of iterations: 779  loss: 0.0032640188\n",
      "Number of iterations: 780  loss: 0.0032224236\n",
      "Number of iterations: 781  loss: 0.005518074\n",
      "Number of iterations: 782  loss: 0.0072861104\n",
      "Number of iterations: 783  loss: 0.017628483\n",
      "Number of iterations: 784  loss: 0.02732298\n",
      "Number of iterations: 785  loss: 0.051071446\n",
      "Number of iterations: 786  loss: 0.03979836\n",
      "Number of iterations: 787  loss: 0.02597828\n",
      "Number of iterations: 788  loss: 0.0060512256\n",
      "Number of iterations: 789  loss: 0.0036033974\n",
      "Number of iterations: 790  loss: 0.0036001198\n",
      "Number of iterations: 791  loss: 0.0032247915\n",
      "Number of iterations: 792  loss: 0.0032736575\n",
      "Number of iterations: 793  loss: 0.00315077\n",
      "Number of iterations: 794  loss: 0.0031080325\n",
      "Number of iterations: 795  loss: 0.003047599\n",
      "Number of iterations: 796  loss: 0.003020857\n",
      "Number of iterations: 797  loss: 0.0029666245\n",
      "Number of iterations: 798  loss: 0.0029405872\n",
      "Number of iterations: 799  loss: 0.002909514\n",
      "Number of iterations: 800  loss: 0.002877005\n",
      "Number of iterations: 801  loss: 0.0028580285\n",
      "Number of iterations: 802  loss: 0.002830227\n",
      "Number of iterations: 803  loss: 0.0028107052\n",
      "Number of iterations: 804  loss: 0.0027908643\n",
      "Number of iterations: 805  loss: 0.002771692\n",
      "Number of iterations: 806  loss: 0.0027552885\n",
      "Number of iterations: 807  loss: 0.0027388507\n",
      "Number of iterations: 808  loss: 0.0027240706\n",
      "Number of iterations: 809  loss: 0.0027100046\n",
      "Number of iterations: 810  loss: 0.0026968499\n",
      "Number of iterations: 811  loss: 0.0026844675\n",
      "Number of iterations: 812  loss: 0.0026727912\n",
      "Number of iterations: 813  loss: 0.0026617493\n",
      "Number of iterations: 814  loss: 0.002651304\n",
      "Number of iterations: 815  loss: 0.0026414043\n",
      "Number of iterations: 816  loss: 0.0026319711\n",
      "Number of iterations: 817  loss: 0.0026230211\n",
      "Number of iterations: 818  loss: 0.0026144618\n",
      "Number of iterations: 819  loss: 0.002606302\n",
      "Number of iterations: 820  loss: 0.0025984717\n",
      "Number of iterations: 821  loss: 0.0025910065\n",
      "Number of iterations: 822  loss: 0.0025837955\n",
      "Number of iterations: 823  loss: 0.002576896\n",
      "Number of iterations: 824  loss: 0.0025702366\n",
      "Number of iterations: 825  loss: 0.0025638298\n",
      "Number of iterations: 826  loss: 0.002557644\n",
      "Number of iterations: 827  loss: 0.0025516595\n",
      "Number of iterations: 828  loss: 0.0025458822\n",
      "Number of iterations: 829  loss: 0.0025402769\n",
      "Number of iterations: 830  loss: 0.0025348403\n",
      "Number of iterations: 831  loss: 0.002529575\n",
      "Number of iterations: 832  loss: 0.0025244404\n",
      "Number of iterations: 833  loss: 0.0025194606\n",
      "Number of iterations: 834  loss: 0.0025145987\n",
      "Number of iterations: 835  loss: 0.0025098727\n",
      "Number of iterations: 836  loss: 0.0025052568\n",
      "Number of iterations: 837  loss: 0.0025007555\n",
      "Number of iterations: 838  loss: 0.0024963438\n",
      "Number of iterations: 839  loss: 0.0024920458\n",
      "Number of iterations: 840  loss: 0.0024878208\n",
      "Number of iterations: 841  loss: 0.002483716\n",
      "Number of iterations: 842  loss: 0.0024796473\n",
      "Number of iterations: 843  loss: 0.0024757015\n",
      "Number of iterations: 844  loss: 0.0024717792\n",
      "Number of iterations: 845  loss: 0.0024679848\n",
      "Number of iterations: 846  loss: 0.0024641936\n",
      "Number of iterations: 847  loss: 0.0024605324\n",
      "Number of iterations: 848  loss: 0.0024568604\n",
      "Number of iterations: 849  loss: 0.0024533237\n",
      "Number of iterations: 850  loss: 0.0024497479\n",
      "Number of iterations: 851  loss: 0.0024463283\n",
      "Number of iterations: 852  loss: 0.0024428377\n",
      "Number of iterations: 853  loss: 0.0024395327\n",
      "Number of iterations: 854  loss: 0.002436102\n",
      "Number of iterations: 855  loss: 0.002432928\n",
      "Number of iterations: 856  loss: 0.002429525\n",
      "Number of iterations: 857  loss: 0.002426505\n",
      "Number of iterations: 858  loss: 0.0024230732\n",
      "Number of iterations: 859  loss: 0.002420252\n",
      "Number of iterations: 860  loss: 0.0024167225\n",
      "Number of iterations: 861  loss: 0.0024141942\n",
      "Number of iterations: 862  loss: 0.0024104142\n",
      "Number of iterations: 863  loss: 0.0024083876\n",
      "Number of iterations: 864  loss: 0.0024040537\n",
      "Number of iterations: 865  loss: 0.0024029275\n",
      "Number of iterations: 866  loss: 0.0023974748\n",
      "Number of iterations: 867  loss: 0.0023981014\n",
      "Number of iterations: 868  loss: 0.002390309\n",
      "Number of iterations: 869  loss: 0.0023945237\n",
      "Number of iterations: 870  loss: 0.0023818512\n",
      "Number of iterations: 871  loss: 0.0023937898\n",
      "Number of iterations: 872  loss: 0.002370855\n",
      "Number of iterations: 873  loss: 0.002400382\n",
      "Number of iterations: 874  loss: 0.0023562755\n",
      "Number of iterations: 875  loss: 0.002429232\n",
      "Number of iterations: 876  loss: 0.0023455834\n",
      "Number of iterations: 877  loss: 0.0025407325\n",
      "Number of iterations: 878  loss: 0.002414316\n",
      "Number of iterations: 879  loss: 0.0030265946\n",
      "Number of iterations: 880  loss: 0.003088452\n",
      "Number of iterations: 881  loss: 0.005469199\n",
      "Number of iterations: 882  loss: 0.0075268033\n",
      "Number of iterations: 883  loss: 0.0176865\n",
      "Number of iterations: 884  loss: 0.026426617\n",
      "Number of iterations: 885  loss: 0.044521905\n",
      "Number of iterations: 886  loss: 0.03134482\n",
      "Number of iterations: 887  loss: 0.017770665\n",
      "Number of iterations: 888  loss: 0.004067949\n",
      "Number of iterations: 889  loss: 0.0029479454\n",
      "Number of iterations: 890  loss: 0.003245386\n",
      "Number of iterations: 891  loss: 0.0028162173\n",
      "Number of iterations: 892  loss: 0.0028354414\n",
      "Number of iterations: 893  loss: 0.0027430465\n",
      "Number of iterations: 894  loss: 0.0027047733\n",
      "Number of iterations: 895  loss: 0.002661098\n",
      "Number of iterations: 896  loss: 0.00263644\n",
      "Number of iterations: 897  loss: 0.0026003933\n",
      "Number of iterations: 898  loss: 0.002577599\n",
      "Number of iterations: 899  loss: 0.002558038\n",
      "Number of iterations: 900  loss: 0.0025330703\n",
      "Number of iterations: 901  loss: 0.00251892\n",
      "Number of iterations: 902  loss: 0.0024996053\n",
      "Number of iterations: 903  loss: 0.0024846024\n",
      "Number of iterations: 904  loss: 0.0024708216\n",
      "Number of iterations: 905  loss: 0.0024567342\n",
      "Number of iterations: 906  loss: 0.002445007\n",
      "Number of iterations: 907  loss: 0.0024332362\n",
      "Number of iterations: 908  loss: 0.0024226413\n",
      "Number of iterations: 909  loss: 0.002412661\n",
      "Number of iterations: 910  loss: 0.0024032756\n",
      "Number of iterations: 911  loss: 0.0023945146\n",
      "Number of iterations: 912  loss: 0.0023862387\n",
      "Number of iterations: 913  loss: 0.0023784498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 914  loss: 0.00237108\n",
      "Number of iterations: 915  loss: 0.0023641163\n",
      "Number of iterations: 916  loss: 0.0023574978\n",
      "Number of iterations: 917  loss: 0.0023512046\n",
      "Number of iterations: 918  loss: 0.0023452162\n",
      "Number of iterations: 919  loss: 0.002339502\n",
      "Number of iterations: 920  loss: 0.002334039\n",
      "Number of iterations: 921  loss: 0.0023288091\n",
      "Number of iterations: 922  loss: 0.002323804\n",
      "Number of iterations: 923  loss: 0.002318981\n",
      "Number of iterations: 924  loss: 0.0023143634\n",
      "Number of iterations: 925  loss: 0.002309905\n",
      "Number of iterations: 926  loss: 0.0023056066\n",
      "Number of iterations: 927  loss: 0.0023014504\n",
      "Number of iterations: 928  loss: 0.0022974338\n",
      "Number of iterations: 929  loss: 0.002293557\n",
      "Number of iterations: 930  loss: 0.0022897844\n",
      "Number of iterations: 931  loss: 0.0022861308\n",
      "Number of iterations: 932  loss: 0.0022825808\n",
      "Number of iterations: 933  loss: 0.00227913\n",
      "Number of iterations: 934  loss: 0.002275773\n",
      "Number of iterations: 935  loss: 0.0022724888\n",
      "Number of iterations: 936  loss: 0.0022692983\n",
      "Number of iterations: 937  loss: 0.002266167\n",
      "Number of iterations: 938  loss: 0.0022631243\n",
      "Number of iterations: 939  loss: 0.0022601332\n",
      "Number of iterations: 940  loss: 0.002257216\n",
      "Number of iterations: 941  loss: 0.0022543469\n",
      "Number of iterations: 942  loss: 0.0022515422\n",
      "Number of iterations: 943  loss: 0.0022487813\n",
      "Number of iterations: 944  loss: 0.002246084\n",
      "Number of iterations: 945  loss: 0.00224341\n",
      "Number of iterations: 946  loss: 0.0022408117\n",
      "Number of iterations: 947  loss: 0.002238216\n",
      "Number of iterations: 948  loss: 0.0022357092\n",
      "Number of iterations: 949  loss: 0.0022331784\n",
      "Number of iterations: 950  loss: 0.0022307588\n",
      "Number of iterations: 951  loss: 0.0022282742\n",
      "Number of iterations: 952  loss: 0.0022259415\n",
      "Number of iterations: 953  loss: 0.0022234998\n",
      "Number of iterations: 954  loss: 0.0022212474\n",
      "Number of iterations: 955  loss: 0.0022188302\n",
      "Number of iterations: 956  loss: 0.0022166793\n",
      "Number of iterations: 957  loss: 0.0022142434\n",
      "Number of iterations: 958  loss: 0.0022122294\n",
      "Number of iterations: 959  loss: 0.0022097134\n",
      "Number of iterations: 960  loss: 0.002207915\n",
      "Number of iterations: 961  loss: 0.0022052175\n",
      "Number of iterations: 962  loss: 0.0022037646\n",
      "Number of iterations: 963  loss: 0.0022006603\n",
      "Number of iterations: 964  loss: 0.0021998892\n",
      "Number of iterations: 965  loss: 0.0021959087\n",
      "Number of iterations: 966  loss: 0.0021964838\n",
      "Number of iterations: 967  loss: 0.002190676\n",
      "Number of iterations: 968  loss: 0.0021940896\n",
      "Number of iterations: 969  loss: 0.002184365\n",
      "Number of iterations: 970  loss: 0.0021940374\n",
      "Number of iterations: 971  loss: 0.0021759323\n",
      "Number of iterations: 972  loss: 0.0022002878\n",
      "Number of iterations: 973  loss: 0.002164612\n",
      "Number of iterations: 974  loss: 0.0022266284\n",
      "Number of iterations: 975  loss: 0.002158519\n",
      "Number of iterations: 976  loss: 0.0023321211\n",
      "Number of iterations: 977  loss: 0.0022382934\n",
      "Number of iterations: 978  loss: 0.0028209575\n",
      "Number of iterations: 979  loss: 0.0029854605\n",
      "Number of iterations: 980  loss: 0.0054467204\n",
      "Number of iterations: 981  loss: 0.008013503\n",
      "Number of iterations: 982  loss: 0.019044718\n",
      "Number of iterations: 983  loss: 0.028304089\n",
      "Number of iterations: 984  loss: 0.042879228\n",
      "Number of iterations: 985  loss: 0.025365928\n",
      "Number of iterations: 986  loss: 0.011283978\n",
      "Number of iterations: 987  loss: 0.0028040921\n",
      "Number of iterations: 988  loss: 0.0026480642\n",
      "Number of iterations: 989  loss: 0.0030122001\n",
      "Number of iterations: 990  loss: 0.0025732166\n",
      "Number of iterations: 991  loss: 0.0025489624\n",
      "Number of iterations: 992  loss: 0.0025033916\n",
      "Number of iterations: 993  loss: 0.0024619573\n",
      "Number of iterations: 994  loss: 0.0024278844\n",
      "Number of iterations: 995  loss: 0.0024061247\n",
      "Number of iterations: 996  loss: 0.0023797823\n",
      "Number of iterations: 997  loss: 0.002358833\n",
      "Number of iterations: 998  loss: 0.0023444046\n",
      "Number of iterations: 999  loss: 0.0023238673\n",
      "Model saves:  ./model4.ckpt\n",
      "The training is finished\n",
      "INFO:tensorflow:Restoring parameters from ./model4.ckpt\n",
      "PMSE: 0.013238471194786891\n",
      "RMSE: 390.33633052770335\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f146e06fe48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([25892.191, 25940.84 , 25929.135, 26038.594, 26180.371, 26209.098,\n",
       "       26296.525, 26436.88 , 26467.314, 26226.79 , 26102.291, 26157.322,\n",
       "       25859.479, 25204.91 , 24359.525, 24839.295, 24692.95 , 24195.42 ,\n",
       "       24657.395, 24620.639, 24531.723, 24808.303, 25116.291, 25037.383,\n",
       "       25012.416, 24858.45 , 25112.76 , 25545.47 , 25662.22 , 25334.584,\n",
       "       24960.152, 24509.809, 24702.834, 24867.158, 24690.69 , 24874.145,\n",
       "       25127.457, 25320.031, 25111.104, 24731.336], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run trainLSTM\n",
    "trainLSTM( batchSize=32, timeStep=20, trainBegin=1000, trainEnd=1300, data=data)\n",
    "# run prediction\n",
    "prediction( timeStep=20, testBegin=1300, trainEnd=1400, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
